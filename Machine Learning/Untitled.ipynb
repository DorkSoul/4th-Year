{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cffb516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Filtering using Naive Bayes\n",
    "#--------------------------------------------------\n",
    "# \n",
    "# Knowing how to classify documents based on their contents is a very\n",
    "# practical application of machine learning. Perhaps the most useful\n",
    "# and well-known application of document filtering is spam detection.\n",
    "# \n",
    "# We will develop a probability based classifier that will identify\n",
    "# spam. You should remember that the naieve bayes algorithms we will\n",
    "# look at today is not specific to dealing with spam. Because it\n",
    "# solves the more general problem of learning to recognize whether\n",
    "# a document belongs in one category or another, it can be used\n",
    "# for less unsavory purposes. \n",
    "#\n",
    "#\n",
    "# Documents, Words and Features\n",
    "#--------------------------------------------------\n",
    "# \n",
    "# The classifier that we will be building needs features to use for\n",
    "# classifying different items.\n",
    "# \n",
    "# A feature is anything that you can determine as being either present\n",
    "# or absent in the item.\n",
    "# \n",
    "# When considering documents for classification, the items are the\n",
    "# documents and the features are the words in the document.\n",
    "# \n",
    "# When using words as features, the assumption is that some words are\n",
    "# more likely to appear in spam than in nonspam, which is the basic\n",
    "# premise underlying most spam filters.\n",
    "# \n",
    "# Features don’t have to be individual words, however; they can be\n",
    "# word pairs or phrases or anything else that can be classified\n",
    "# as absent or present in a particular document.\n",
    "#\n",
    "# They could even be whole documents or single letters. Can you think\n",
    "# why using whole documents or single letters as features typically\n",
    "# isn't a good idea?\n",
    "#\n",
    "# ------------------Python Interlude------------------------------------\n",
    "# As part of the preprocessing of the text data we will\n",
    "# use the Python re module. This module allows us to create\n",
    "# regular expressions that you can use to split up text.\n",
    "#\n",
    "# The following illustrates how to use re to split up text\n",
    "# into a list of words:\n",
    "#\n",
    "## \n",
    "## import re\n",
    "## splitter=re.compile('\\W+')\n",
    "## s1 = 'The quick brown fox jumped over the lazy dog.''\n",
    "## s2 = splitter.split(s1)\n",
    "\n",
    "## s2\n",
    "## ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '']\n",
    "## \n",
    "# \n",
    "# re.compile() creates a regular expression.\n",
    "# The parameter \\ introduces a special sequence\n",
    "# The parameter \\W is a special sequence that matches any non-alphanumeric character\n",
    "# \n",
    "# See http://docs.python.org/library/re.html for doc on re\n",
    "# \n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "import re #regular expression library\n",
    "import math\n",
    "\n",
    "\n",
    "# The first thing we need is a function that will extract the features (words)\n",
    "# from a text. The getwords features will do this for us.\n",
    "# This function breaks up the text into words by dividing the text on\n",
    "# any character that isn’t a letter. This leaves only actual words,\n",
    "# all converted to lowercase.\n",
    "def getwords(doc):\n",
    "  splitter=re.compile('\\W+') \n",
    "\n",
    "  # Split the words by non-alpha characters\n",
    "  # Exclude words with a length 2 character or less or\n",
    "  # greater than 20 (words longer the 20 letters are\n",
    "  # likely to be either errors in the splitting or\n",
    "  # so rare as to be useless for classifying\n",
    "  words = [s.lower() for s in splitter.split(doc) \n",
    "                if len(s)>2 and len(s)<20]\n",
    "  \n",
    "  # Return the unique set of words only\n",
    "  return dict([(w,1) for w in words])\n",
    "\n",
    "# \n",
    "# Determining which features to use is both very tricky and very important.\n",
    "# - The features must be common enough that they appear frequently,\n",
    "# - but not so common that they appear in every single document.\n",
    "# \n",
    "# The other thing to consider when deciding on features is how well they\n",
    "# will divide the set of documents into the target categories.\n",
    "# \n",
    "# For example, the code for getwords above reduces the total number of\n",
    "# features by converting them to lowercase. This means it will recognize\n",
    "# that a capitalized word at the start of a sentence is the same\n",
    "# as when that word is all lowercase in the middle of a sentence—a good\n",
    "# thing, since words with different capitalization usually have the same\n",
    "# meaning. However, this function will completely miss the SHOUTING style\n",
    "# used in many spam messages, which may be vital for dividing the set into\n",
    "# spam and nonspam. \n",
    "# \n",
    "# As you can see, the choice of feature set involves many trade-offs and\n",
    "# is subject to endless tweaking. For now, we can use the simple\n",
    "# getwords function given.\n",
    "# \n",
    "# Lets try out getwords in our python session:\n",
    "# \n",
    "# >>> getwords('this is a test sentence')\n",
    "# this is a test sentence\n",
    "# {'this': 1, 'test': 1, 'sentence': 1}\n",
    "# \n",
    "# getwords('this is this')\n",
    "# this is this\n",
    "# {'this': 1}\n",
    "# \n",
    "#\n",
    "#\n",
    "# Training the Classifier\n",
    "#--------------------------------------------------\n",
    "# \n",
    "# We need to make a class to represent the classifier. \n",
    "\n",
    "class classifier:\n",
    "  def __init__(self, getfeatures, filename=None):\n",
    "    # Counts of feature/category combinations\n",
    "    self.fc = {}\n",
    "    # Counts of documents in each category\n",
    "    self.cc = {}\n",
    "    self.getfeatures = getfeatures\n",
    "\n",
    "# The fc variable will store the counts for different features in\n",
    "# different classifications.\n",
    "# - For example:\n",
    "#   {'python': {'bad': 0, 'good': 6}, 'the': {'bad': 3, 'good': 3}}\n",
    "# This indicates that the word “the” has appeared in documents\n",
    "# classified as bad three times, and in documents that were\n",
    "# classified as good three times. The word “Python”\n",
    "# has only appeared in good documents.\n",
    "# \n",
    "# The cc variable is a dictionary of how many times every\n",
    "# classification has been used. This is needed for the probability\n",
    "# calculations that we’ll discuss shortly.\n",
    "# \n",
    "# The final instance variable, getfeatures, is the function that will\n",
    "# be used to extract the features from the items being classified\n",
    "# —in this example, it is the getwords function wejust defined.\n",
    "\n",
    "# \n",
    "# We also need helper methods to increment and get the counts of\n",
    "# the features\n",
    "# \n",
    "    \n",
    "  # Increase the count of a feature/category pair\n",
    "  def incf(self,f,cat):\n",
    "    self.fc.setdefault(f,{}) \n",
    "    self.fc[f].setdefault(cat,0)\n",
    "    self.fc[f][cat]+=1\n",
    "\n",
    "  # Increase the count of a category\n",
    "  def incc(self,cat):\n",
    "    self.cc.setdefault(cat,0)\n",
    "    self.cc[cat]+=1\n",
    "\n",
    "  # The number of times a feature has appeared in a category\n",
    "  def fcount(self,f,cat):\n",
    "    if f in self.fc and cat in self.fc[f]:\n",
    "      return float(self.fc[f][cat])\n",
    "    return 0.0\n",
    "\n",
    "  # The number of items in a category\n",
    "  def catcount(self,cat):\n",
    "    if cat in self.cc:\n",
    "      return float(self.cc[cat])\n",
    "    return 0\n",
    "\n",
    "  # The total number of items\n",
    "  def totalcount(self):\n",
    "    return sum(self.cc.values())\n",
    "\n",
    "  # The list of all categories\n",
    "  def categories(self):\n",
    "    return self.cc.keys()\n",
    "\n",
    "\n",
    "# The train method takes an item (a document in this case) and a\n",
    "# classification.\n",
    "# \n",
    "# It uses the getfeatures function of the class to break the item\n",
    "# into its separate features.\n",
    "# \n",
    "# It then calls incf to increase the counts for this classification\n",
    "# for every feature.\n",
    "# \n",
    "# Finally, it increases the total count for this classification:\n",
    "\n",
    "  def train(self,item,cat):\n",
    "    features=self.getfeatures(item)\n",
    "    # Increment the count for every feature with this category\n",
    "    for f in features:\n",
    "      self.incf(f,cat)\n",
    "    # Increment the count for this category\n",
    "    self.incc(cat)\n",
    "\n",
    "\n",
    "\n",
    "# Lets check if the train method updates the counts correctly:\n",
    "#\n",
    "# cl = classifier(getwords)\n",
    "# cl.train('the quick brown fox jumped over the lazy dog', 'good')\n",
    "# the quick brown fox jumped over the lazy dog\n",
    "\n",
    "# cl.train('make quick money in the online casino', 'bad')\n",
    "# make quick money in the online casino\n",
    "# \n",
    "# cl.fcount('quick','good')\n",
    "# 1.0\n",
    "# >>> cl.fcount('quick','bad')\n",
    "# 1.0\n",
    "# >>> cl.fcount('jumped','bad')\n",
    "# 0.0\n",
    "# >>> \n",
    "#\n",
    "# The fcount function returns the number of times a feature has\n",
    "# appeared in a category.\n",
    "\n",
    "\n",
    "# To save ourselves the bother of continually typing in the training\n",
    "# data we can add a method to do this automatically.\n",
    "# The sampletrain() method at the end of the file does this for us.\n",
    "#\n",
    "\n",
    "\n",
    "# Calculating Probabilities\n",
    "#--------------------------------------------------\n",
    "# \n",
    "# We now have counts for how often an feature appears in each category,\n",
    "# so the next step is to convert these numbers into probabilities.\n",
    "# \n",
    "# A probability is a number between 0 and 1, indicating how likely an\n",
    "# event is.\n",
    "# \n",
    "# We can calculate the probability that a word (feature)\n",
    "# is in a particular category by dividing the number of times\n",
    "# the word appears in a document in that category by the total number\n",
    "# of documents in that category.\n",
    "# \n",
    "# The method fprob does this calculation for us:\n",
    "\n",
    "  def fprob(self,f,cat):\n",
    "    if self.catcount(cat)==0: return 0\n",
    "\n",
    "    # The total number of times this feature appeared in this \n",
    "    # category divided by the total number of items in this category\n",
    "    return self.fcount(f,cat)/self.catcount(cat)\n",
    "\n",
    "# The number returned by fprob is called a conditional probability.\n",
    "# Conditional probabilities are usually written as Pr(A | B) and read as\n",
    "# “the probability of A given B.”\n",
    "# \n",
    "# In this example, the numbers we have now are Pr(word | classification);\n",
    "# that is, for a given classification we calculate the probability\n",
    "# that a particular word appears.\n",
    "# \n",
    "# We can test this in our Python session:\n",
    "#   \n",
    "# >>> cl = classifier(getwords)\n",
    "# >>> sampletrain(cl)\n",
    "# Nobody owns the water.\n",
    "# the quick rabbit jumps fences\n",
    "# buy pharmaceuticals now\n",
    "# make quick money at the online casino\n",
    "# the quick brown fox jumps\n",
    "# >>> cl.fprob('quick','good')\n",
    "# 0.66666666666666663\n",
    "# >>> \n",
    "# \n",
    "# The word “quick” appears in two of the three documents classified\n",
    "# as good, which means there’s a probability of\n",
    "# Pr(quick| good) = 0.666 (a 2/3 chance)\n",
    "# that a good document will contain that word.\n",
    "\n",
    "\n",
    "# \n",
    "# Starting with a Reasonable Guess\n",
    "#--------------------------------------------------\n",
    "# \n",
    "# The fprob method gives an accurate result for the features and\n",
    "# classifications it has seen so far, but it has a slight problem\n",
    "#   —using only the information it has seen so far makes it\n",
    "#   incredibly sensitive during early training and to words that\n",
    "#   appear very rarely.\n",
    "# \n",
    "#   In the sample training data, the word “money” only appears in\n",
    "#   one document and is classified as bad because it is a casino ad.\n",
    "#   Since the word “money” is in one bad document and no good ones,\n",
    "#   the probability that it will appear in the good category using\n",
    "#   fprob is now 0.\n",
    "#   This is a bit extreme, since “money” might be a perfectly neutral\n",
    "#   word that just happens to appear first in a bad document. It\n",
    "#   would be much more realistic for the value to gradually approach\n",
    "#   zero as a word is found in more and more documents with the\n",
    "#   same category.\n",
    "# \n",
    "# To get around this, we need to decide on an assumed probability,\n",
    "# which will be used when we have very little information about\n",
    "# the feature in question.\n",
    "# \n",
    "# A good number to start with is 0.5.\n",
    "# \n",
    "# We also need to decide how much to weight the assumed probability\n",
    "#   —a weight of 1 means the assumed probability is weighted the\n",
    "#   same as one word.\n",
    "# \n",
    "# The weighted probability returns a weighted average of\n",
    "# getprobability and the assumed probability.\n",
    "#   -  In the “money” example, the weighted probability for the\n",
    "#   word “money” starts at 0.5 for all categories.\n",
    "#   After the classifier is trained with one bad document and finds\n",
    "#   that “money” fits into the bad category, its probability becomes\n",
    "#   0.75 for bad. This is because:\n",
    "# \n",
    "#   (weight*assumedprob + count*fprob)/(count+weight)\n",
    "#   = (1*1.0+1*0.5+)/(1.0 + 1.0)\n",
    "#   = 0.75\n",
    "# \n",
    "# \n",
    "# >>> cl = classifier(getwords)\n",
    "# >>> cl.train('make quick money in the online casino','bad')\n",
    "# make quick money in the online casino\n",
    "# >>> cl.fcount('money','bad')\n",
    "# 1.0\n",
    "# >>> cl.catcount('bad')\n",
    "# 1.0\n",
    "# >>> cl.fprob('money','bad')\n",
    "# 1.0\n",
    "# \n",
    "\n",
    "\n",
    "  def weightedprob(self,f,cat,prf,weight=1.0,ap=0.5):\n",
    "    # Calculate current probability\n",
    "    basicprob=prf(f,cat)\n",
    "\n",
    "    # Count the number of times this feature has appeared in\n",
    "    # all categories\n",
    "    totals=sum([self.fcount(f,c) for c in self.categories()])\n",
    "\n",
    "    # Calculate the weighted average\n",
    "    bp=((weight*ap)+(totals*basicprob))/(weight+totals)\n",
    "    return bp\n",
    "\n",
    "# We can see how the probabilities change as more examples are\n",
    "# given to the classifier by rerunning the sampletrain method.\n",
    "# \n",
    "# >>> cl.weightedprob('money','bad',cl.fprob,1.0,0.5)\n",
    "# 0.75\n",
    "\n",
    "# >>> cl = classifier(getwords)\n",
    "# >>> sampletrain(cl)\n",
    "# Nobody owns the water.\n",
    "# the quick rabbit jumps fences\n",
    "# buy pharmaceuticals now\n",
    "# make quick money at the online casino\n",
    "# the quick brown fox jumps\n",
    "# >>> cl.weightedprob('money','good',cl.fprob)\n",
    "# 0.25\n",
    "# \n",
    "# >>> sampletrain(cl)\n",
    "# Nobody owns the water.\n",
    "# the quick rabbit jumps fences\n",
    "# buy pharmaceuticals now\n",
    "# make quick money at the online casino\n",
    "# the quick brown fox jumps\n",
    "# >>> cl.weightedprob('money','good',cl.fprob)\n",
    "# 0.16666666666666666\n",
    "# >>> \n",
    "# \n",
    "# You can see how the classifier becomes more confident of the\n",
    "# various word probabilities as they get pulled further from their\n",
    "# assumed probability.\n",
    "# \n",
    "# The assumed probability of 0.5 was chosen simply because it is\n",
    "# halfway between 0 and 1. However, it’s possible that you might\n",
    "# have better background information than that, even on a completely\n",
    "# untrained classifier.\n",
    "# \n",
    "\n",
    "\n",
    "# A Naïve Classifier\n",
    "#--------------------------------------------------\n",
    "# \n",
    "# \n",
    "# Once we have the probabilities of a document in a category\n",
    "# containing a particular word, you need a way to combine the\n",
    "# individual word probabilities to get the probability that\n",
    "# an entire document belongs in a given category.\n",
    "# \n",
    "# The classifier we will use to do this is called a naïve\n",
    "# Bayesian classifier.\n",
    "# \n",
    "# It is called naïve because it assumes that the probabilities being\n",
    "# combinedare independent of each other.\n",
    "#   - That is, the probability of one word in the document being in a\n",
    "#   specific category is unrelated to the probability of the other words\n",
    "#   being in that category.\n",
    "# \n",
    "# This is actually a false assumption:\n",
    "#   -you’ll probably find that documents containing the word “casino”\n",
    "#   are much more likely to contain the word “money” than documents\n",
    "#   about Python programming are.\n",
    "# \n",
    "# This means that you can’t actually use the probability created by\n",
    "# the naïve Bayesian classifier as the actual probability that a\n",
    "# document belongs in a category, because the assumption of\n",
    "# independence makes it inaccurate.\n",
    "# \n",
    "# However, you can compare the results for different categories and\n",
    "# see which one has the highest probability. In real life, despite\n",
    "# the underlying flawed assumption, this has proven to be a\n",
    "# surprisingly effective method for classifying documents.\n",
    "\n",
    "# Probability of a Whole Document\n",
    "#--------------------------------------------------\n",
    "# \n",
    "# To use the naïve Bayesian classifier, we first have to determine\n",
    "# the probability of an entire document being given a classification.\n",
    "# \n",
    "# We are going to assume the probabilities are independent,\n",
    "# which means we can calculate the probability of all of them by\n",
    "# multiplying them together.\n",
    "# \n",
    "#   - For example, suppose you’ve noticed that the word “Python”\n",
    "#   appears in 20 percent of your bad documents\n",
    "# \n",
    "#     Pr(Python | Bad) = 0.2\n",
    "#     \n",
    "#   and that the word “casino” appears in 80 percent of your bad\n",
    "#   documents\n",
    "# \n",
    "#   (Pr(Casino | Bad) = 0.8)\n",
    "# \n",
    "#   You would then expect the independent probability of both\n",
    "#   words appearing in a bad document to be:\n",
    "#     \n",
    "#   Pr(Python & Casino | Bad)—to be 0.8 × 0.2 = 0.16.\n",
    "# \n",
    "# From this you can see that calculating the entire document\n",
    "# probability is just a matter of multiplying together all\n",
    "# the probabilities of the individual words in that document.\n",
    "# \n",
    "# We will create a subclass of classifier called naivebayes,\n",
    "# and create a docprob method that extracts the features (words)\n",
    "# and multiplies all their probabilities together to get an\n",
    "# overall probability:\n",
    "\n",
    "\n",
    "class naivebayes(classifier):\n",
    "  \n",
    "  def __init__(self, getfeatures):\n",
    "    classifier.__init__(self, getfeatures)\n",
    "    #used the hold the thresholds for each category\n",
    "    #(this is explained later in the lab)\n",
    "    self.thresholds={}\n",
    "  \n",
    "  def docprob(self, item, cat):\n",
    "    features=self.getfeatures(item)   \n",
    "\n",
    "    # Multiply the probabilities of all the features together\n",
    "    p = 1 \n",
    "    for f in features: p*=self.weightedprob(f, cat, self.fprob)\n",
    "    return p\n",
    "\n",
    "# We know how to calculate Pr(Document | Category), but this\n",
    "# isn’t very useful by itself.\n",
    "# \n",
    "# In order to classify documents, we really need:\n",
    "#   Pr(Category | Document).\n",
    "#   \n",
    "# In other words, given a specific document, what’s the\n",
    "# probability that it fits into this category?\n",
    "# \n",
    "# Fortunately, a British mathematician named Thomas Bayes\n",
    "# figured out how to do this about 250 years ago.\n",
    "# \n",
    "# Bayes’ Theorem is a way of flipping around conditional\n",
    "# probabilities. It’s usually written as:\n",
    "#   \n",
    "#   Pr(A | B) = Pr(B | A) x Pr(A)/Pr(B)\n",
    "# \n",
    "# In the example, this becomes:\n",
    "# \n",
    "#   Pr(Category | Document) =\n",
    "#     Pr(Document | Category) x Pr(Category) / Pr(Document)\n",
    "# \n",
    "# We know how to calculate Pr(Document | Category),\n",
    "# but what about the other two values in the equation?\n",
    "# \n",
    "# Well, Pr(Category) is the probability that a randomly\n",
    "# selected document will be in this category, so it’s just\n",
    "# the number of documents in the category divided by the\n",
    "# total number of documents.\n",
    "# \n",
    "# As for Pr(Document), we could calculate it, but that\n",
    "# would be unnecessary effort. Remember that the results of\n",
    "# this calculation will not be used as a real probability.\n",
    "# Instead, the probability for each category will be\n",
    "# calculated separately, and then all the results will be\n",
    "# compared.\n",
    "# Since Pr(Document) is the same no matter what category\n",
    "# the calculation is being done for, it will scale the\n",
    "# results by the exact same amount, so we can safely\n",
    "# ignore this term.\n",
    "# \n",
    "# The prob method calculates the probability of the\n",
    "# category, and returns the product of\n",
    "# Pr(Document | Category) and Pr(Category). \n",
    "\n",
    "  def prob(self,item,cat):\n",
    "    catprob=self.catcount(cat)/self.totalcount()\n",
    "    docprob=self.docprob(item,cat)\n",
    "    return docprob*catprob\n",
    "\n",
    "\n",
    "# Lets try our Naieve Bayes classifier out in out\n",
    "# Python session:\n",
    "# \n",
    "# >>> cl = naivebayes(getwords)\n",
    "# >>> sampletrain(cl)\n",
    "# Nobody owns the water.\n",
    "# the quick rabbit jumps fences\n",
    "# buy pharmaceuticals now\n",
    "# make quick money at the online casino\n",
    "# the quick brown fox jumps\n",
    "# >>> cl.prob('quick rabbit','good')\n",
    "# quick rabbit\n",
    "# 0.15624999999999997\n",
    "# >>> cl.prob('quick rabbit','bad')\n",
    "# quick rabbit\n",
    "# 0.050000000000000003\n",
    "# >>> \n",
    "# \n",
    "# Based on the training data, the phrase “quick rabbit”\n",
    "# is considered a much better candidate for the good\n",
    "# category than the bad.\n",
    "\n",
    "\n",
    "# Choosing a Category\n",
    "# \n",
    "# The final step in building the naïve Bayes classifier is\n",
    "# actually deciding in which category a new item belongs.\n",
    "# \n",
    "# The simplest approach would be to calculate the probability\n",
    "# of this item being in each of the different categories and\n",
    "# to choose the category with the best probability.\n",
    "# \n",
    "# If you were just trying to decide the best place to put something,\n",
    "# this would be a feasible strategy, but\n",
    "#   - in many applications the categories can’t be considered equal,\n",
    "#   - and in some applications it’s better for the classifier to\n",
    "#   admit that it doesn’t know the answer than to decide that the\n",
    "#   answer is the category with a marginally higher probability.\n",
    "# \n",
    "# In the case of spam filtering, it’s much more important to avoid\n",
    "# having good email messages classified as spam than it is to catch\n",
    "# every single spam message.\n",
    "# \n",
    "# To deal with this problem, we can set up a minimum threshold for\n",
    "# each category. For a new item to be classified into a particular\n",
    "# category, its probability must be a specified amount larger than\n",
    "# the probability for any other category. This specified\n",
    "# amount is the threshold.\n",
    "#   - For spam filtering, the threshold to be filtered to bad could\n",
    "#   be 3, so that the probability for bad would have to be 3 times\n",
    "#   higher than the probability for good.\n",
    "#   - The threshold for good could be set to 1, so anything would\n",
    "#   be good if the probability were at all better than for the bad\n",
    "#   category.\n",
    "#   - Any message where the probability for bad is higher, but not\n",
    "#   3 times higher, would be classified as unknown.\n",
    "# \n",
    "# We will use the thresholds instance variable declared in the class\n",
    "# init function to hold these threshols.\n",
    "# \n",
    "# We will also add some methods to set and get the threshold values,\n",
    "# returning 1.0 as default:\n",
    "  \n",
    "  def setthreshold(self,cat,t):\n",
    "    self.thresholds[cat]=t\n",
    "    \n",
    "  def getthreshold(self,cat):\n",
    "    if cat not in self.thresholds: return 1.0\n",
    "    return self.thresholds[cat]\n",
    "\n",
    "# We can now implement a classify method.\n",
    "# \n",
    "# This method will:\n",
    "#   (1) calculate the probability for each category,\n",
    "#   (2) determine which one is the largest\n",
    "#   (3) determine whether it exceeds the next largest\n",
    "#   by more than its threshold.\n",
    "#   (4) If none of the categories can accomplish this, the\n",
    "#   method just returns the default values.\n",
    " \n",
    "  def classify(self,item,default=None):\n",
    "    probs={}\n",
    "    # Find the category with the highest probability\n",
    "    max=0.0\n",
    "    for cat in self.categories():\n",
    "      probs[cat]=self.prob(item,cat)\n",
    "      if probs[cat]>max: \n",
    "        max=probs[cat]\n",
    "        best=cat\n",
    "\n",
    "    # Make sure the probability exceeds threshold*next best\n",
    "    for cat in probs:\n",
    "      if cat==best: continue\n",
    "      if probs[cat]*self.getthreshold(best)>probs[best]: return default\n",
    "    return best\n",
    "\n",
    "# Thats everything we need for a Naieve Bayes classifier!\n",
    "# \n",
    "# Lets try it out:\n",
    "# \n",
    "# >>> cl = naivebayes(getwords)\n",
    "# >>> sampletrain(cl)\n",
    "# >>> cl.classify('quick rabbit',default='unknown')\n",
    "# 'good'\n",
    "# >>> cl.classify('quick money',default='unknown')\n",
    "# 'bad'\n",
    "# >>> cl.setthreshold('bad',3.0)\n",
    "# >>> cl.classify('quick money',default='unknown')\n",
    "# 'unknown'\n",
    "# >>> for i in range(10): sampletrain(cl)\n",
    "# ...\n",
    "# >>> cl.classify('quick money',default='unknown')\n",
    "# 'bad'\n",
    "# \n",
    "# \n",
    "# We could extend our classifier to use different features.\n",
    "# \n",
    "# We can also alter the thresholds and see how\n",
    "# the results are affected.\n",
    "# \n",
    "# The thresholds will also be different for other\n",
    "# applications that involve document filtering; sometimes a\n",
    "# ll categories will be equal, or filtering to “unknown” will\n",
    "# be unacceptable.\n",
    "\n",
    "\n",
    "# dump some sample training data in a classifier\n",
    "def sampletrain(cl):\n",
    "  cl.train('Nobody owns the water.','good')\n",
    "  cl.train('the quick rabbit jumps fences','good')\n",
    "  cl.train('buy pharmaceuticals now','bad')\n",
    "  cl.train('make quick money at the online casino','bad')\n",
    "  cl.train('the quick brown fox jumps','good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814cc56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
